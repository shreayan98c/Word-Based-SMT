#!/usr/bin/env python
import re
import sys
import json
import optparse

# option parser
optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=100000000000, type="int",
                     help="Number of sentences to use for training and alignment")
optparser.add_option("-i", "--iterations", dest="iterations", default=5, type="int",
                     help="Number of iterations to train the IBM Model 1 + EM Algo on (default=5)")
optparser.add_option("-t", "--thresh", dest="threshold", default=0.5, type="int",
                     help="Threshold value for the aligning words p(e|f) and p(f|e) (default=0.5)")
(opts, _) = optparser.parse_args()

# filepath to the data
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)


def uniform_trans_prob_initialization(parallel_corpus: list, is_reverse: bool = False) -> tuple[list, dict, set, set]:
    """
    Function to initialize the translation probabilities by uniform distribution.
    :param: parallel_corpus: The sentences in source and foreign language.
    :param: is_reverse: Whether to calculate p(e|f) or p(f|e).
    :return: initial translation probabilities, f_vocab, e_vocab
    """
    parallel_corpus = parallel_corpus[: opts.num_sents]
    e_vocab = set()
    f_vocab = set()
    trans_probs = dict()

    # first create the source and transition vocabs
    for n, (f_sent, e_sent) in enumerate(parallel_corpus):
        # n - sentence number in the parallel corpus
        # f_sent - list containing words from the nth sentence in foreign language
        # e_sent - list containing words from the nth sentence in english language
        for f_word in f_sent:
            f_vocab.add(f_word)
        for e_word in e_sent:
            e_vocab.add(e_word)

    source_vocab_size = len(f_vocab)
    target_vocab_size = len(e_vocab)

    if is_reverse:
        for f_word in f_vocab:
            trans_probs[f_word] = dict()
            for e_word in e_vocab:
                trans_probs[f_word][e_word] = 1 / target_vocab_size

        return parallel_corpus, trans_probs, f_vocab, e_vocab

    # calculating init probs
    for e_word in e_vocab:
        trans_probs[e_word] = dict()
        for f_word in f_vocab:
            trans_probs[e_word][f_word] = 1 / source_vocab_size

    return parallel_corpus, trans_probs, f_vocab, e_vocab


def converge_and_optimize(parallel_corpus: list, f_vocab: set, e_vocab: set, trans_probs: dict,
                          n_iterations: int, is_reverse: bool = False) -> dict:
    """
    Function to converge and optimize the learning for the translation probabilities t(e|f)
    :param parallel_corpus: Sentences in source and target language
    :param f_vocab: The vocab set for source language
    :param e_vocab: The vocab set for target language
    :param trans_probs: The initial translation probabilities for the word pairs
    :param n_iterations: Number of iterations to optimize the cost function
    :param is_reverse: Whether to calculate p(e|f) or p(f|e)
    :return: s_total: The total sum of the cost function
    """
    e_vocab = list(e_vocab)
    f_vocab = list(f_vocab)

    if is_reverse:
        for itr_n in range(n_iterations):
            counts = dict()
            total_f = dict()

            # initializing count(e|f) = 0 for all e, f in vocab
            for f_word in f_vocab:
                counts[f_word] = dict()
                for e_word in e_vocab:
                    counts[f_word][e_word] = 0

            # initializing total(f) for all f in vocab
            for e_word in e_vocab:
                total_f[e_word] = 0

            # normalize the probabilities
            for n, (f_sent, e_sent) in enumerate(parallel_corpus):
                s_total = dict()
                for f_word in f_sent:
                    s_total[f_word] = 0
                    for e_word in e_sent:
                        s_total[f_word] += trans_probs[f_word][e_word]

                # collect counts
                for f_word in f_sent:
                    for e_word in e_sent:
                        counts[f_word][e_word] += trans_probs[f_word][e_word] / s_total[f_word]
                        total_f[e_word] += trans_probs[f_word][e_word] / s_total[f_word]

            # estimate probabilities
            for e_word in e_vocab:
                for f_word in f_vocab:
                    trans_probs[f_word][e_word] = counts[f_word][e_word] / total_f[e_word]

        return trans_probs

    for itr_n in range(n_iterations):
        counts = dict()
        total_f = dict()

        # initializing count(e|f) = 0 for all e, f in vocab
        for e_word in e_vocab:
            counts[e_word] = dict()
            for f_word in f_vocab:
                counts[e_word][f_word] = 0

        # initializing total(f) for all f in vocab
        for f_word in f_vocab:
            total_f[f_word] = 0

        # normalize the probabilities
        for n, (f_sent, e_sent) in enumerate(parallel_corpus):
            s_total = dict()
            for e_word in e_sent:
                s_total[e_word] = 0
                for f_word in f_sent:
                    s_total[e_word] += trans_probs[e_word][f_word]

            # collect counts
            for e_word in e_sent:
                for f_word in f_sent:
                    counts[e_word][f_word] += trans_probs[e_word][f_word] / s_total[e_word]
                    total_f[f_word] += trans_probs[e_word][f_word] / s_total[e_word]

        # estimate probabilities
        for f_word in f_vocab:
            for e_word in e_vocab:
                trans_probs[e_word][f_word] = counts[e_word][f_word] / total_f[f_word]

    return trans_probs


def generate_f2e(parallel_corpus: list) -> dict:
    """
    Function to generate f2e from the data
    :param parallel_corpus: Sentences in source and target language
    :return: f2e matrix
    """
    n_iterations = opts.iterations
    parallel_corpus, trans_probs, f_vocab, e_vocab = uniform_trans_prob_initialization(parallel_corpus)
    trans_probs = converge_and_optimize(parallel_corpus, f_vocab, e_vocab, trans_probs, n_iterations)
    return trans_probs


def generate_e2f(parallel_corpus: list) -> dict:
    """
    Function to generate e2f from the data
    :param parallel_corpus: Sentences in source and target language
    :return: e2f matrix
    """
    n_iterations = opts.iterations
    parallel_corpus, trans_probs, f_vocab, e_vocab = uniform_trans_prob_initialization(parallel_corpus,
                                                                                       is_reverse=True)
    trans_probs = converge_and_optimize(parallel_corpus, f_vocab, e_vocab, trans_probs, n_iterations, is_reverse=True)
    return trans_probs


def initialize_matrix(rows: int, columns: int, value: int) -> list:
    """
    A function that to initialize a 2d matrix of the desired shape with the given value
    :param rows: Number of rows
    :param rows: Number of columns
    :param rows: Value to be populated
    :return: e2f matrix
    """
    result = [[value for _ in range(columns)] for _ in range(rows)]
    return result


def intersection(e2f, f2e):
    """
    Function to find out intersection between e2f and f2e
    :param e2f: e2f matrix calculated using IBM Model 1
    :param f2e: f2e matrix calculated using IBM Model 1
    :return: Intersection of the 2 matrices
    """
    rows = len(e2f)
    columns = len(f2e)
    result = initialize_matrix(rows, columns, False)
    for e in range(rows):
        for f in range(columns):
            result[e][f] = e2f[e][f] and f2e[f][e]
    return result


def union(e2f, f2e):
    """
    Function to find out union between e2f and f2e
    :param e2f: e2f matrix calculated using IBM Model 1
    :param f2e: f2e matrix calculated using IBM Model 1
    :return: Union of the 2 matrices
    """
    rows = len(e2f)
    columns = len(f2e)
    result = initialize_matrix(rows, columns, False)
    for e in range(rows):
        for f in range(columns):
            result[e][f] = e2f[e][f] or f2e[f][e]
    return result


def neighboring_points(e_index: int, f_index: int, e_len: int, f_len: int) -> list:
    """
    A function that to find out the list of neighboring points in an alignment matrix for a given pair of indexes.
    :param: e_index: index of e word
    :param: f_index: index of f word
    :param: e_len: length of e seq
    :param: f_len: length of f seq
    :return: List of neighbouring points
    """
    result = []

    if e_index > 0:
        result.append((e_index - 1, f_index))
    if f_index > 0:
        result.append((e_index, f_index - 1))
    if e_index < e_len - 1:
        result.append((e_index + 1, f_index))
    if f_index < f_len - 1:
        result.append((e_index, f_index + 1))
    if e_index > 0 and f_index > 0:
        result.append((e_index - 1, f_index - 1))
    if e_index > 0 and f_index < f_len - 1:
        result.append((e_index - 1, f_index + 1))
    if e_index < e_len - 1 and f_index > 0:
        result.append((e_index + 1, f_index - 1))
    if e_index < e_len - 1 and f_index < f_len - 1:
        result.append((e_index + 1, f_index + 1))

    return result


def grow_diag(union: list, alignment: list, e_len: int, f_len: int) -> None:
    new_points_added = True
    while new_points_added:
        new_points_added = False
        for e in range(e_len):
            for f in range(f_len):
                if alignment[e][f]:
                    for (e_new, f_new) in neighboring_points(e, f, e_len, f_len):
                        if union[e_new][f_new]:
                            alignment[e_new][f_new] = True
                            new_points_added = True


if __name__ == '__main__':

    thresh = opts.threshold
    full_parallel_corpus = [[sentence.lower().strip().split() for sentence in pair] for pair in
                            zip(open(f_data), open(e_data))]
    training_parallel_corpus = full_parallel_corpus[:opts.num_sents]

    f2e = generate_f2e(training_parallel_corpus)
    e2f = generate_e2f(training_parallel_corpus)

    # print(json.dumps(f2e, sort_keys=True, indent=4))
    # print(json.dumps(e2f, sort_keys=True, indent=4))

    word_matches = []
    for e_word, f_words in f2e.items():
        for f_word, prob in f_words.items():
            if prob > thresh:
                word_matches.append((f_word, e_word))

    for f_word, e_words in e2f.items():
        for e_word, prob in e_words.items():
            if prob > thresh:
                word_matches.append((f_word, e_word))

    # predicting on given data
    for n, (f_sent, e_sent) in enumerate(full_parallel_corpus):
        # print(f_sent, e_sent)
        for f_i, f_word in enumerate(f_sent):
            for e_i, e_word in enumerate(e_sent):
                if f_word in e2f and e_word in e2f[f_word] and e2f[f_word][e_word] > thresh:
                    sys.stdout.write("%i-%i " % (f_i, e_i))
        sys.stdout.write("\n")
